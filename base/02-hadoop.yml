---

- name: prepare dynamic properties
  hosts: localhost
  become: no
  vars:
    ssh_users:
      - hadoop
  tasks:

    - name: remove keys directory contents
      local_action: file path=keys state=absent

    - name: create keys directory
      local_action: file path=keys state=directory

    - name: generate ssh key for user
      local_action: command ssh-keygen -t rsa -b 4096 -N "" -f "keys/{{ item }}"
      with_items: "{{ ssh_users }}"

- name: configure users and limits
  hosts: hadoopnodes
  become: yes
  roles:

    - role: common

      users_managed:
        hadoop:
          groups: hadoopadmin
          append: yes

      authorized_keys:
        - user: hadoop
          key: "{{ lookup('file', '../keys/hadoop.pub') }}"
        - user: hadoop
          key: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"

      private_keys:
        - user: hadoop
          key: "{{ lookup('file', '../keys/hadoop') }}"

      directories:
        /hdfs:
          owner: hadoop
          group: hadoop

    - role: limits

      limits_conf_d_files:
        hadoop.conf:
          - domain: hadoop
            type: soft
            item: nofile
            value: 65535
          - domain: hadoop
            type: hard
            item: nofile
            value: 65535


- name: storage
  hosts: datanodes
  become: yes
  tasks:

    # - name: storage | create partition label
    #   command: parted -s /dev/vdb mklabel gpt creates=/dev/vdb1
    #   tags: storage

    # - name: storage | create partition
    #   command: parted -s /dev/vdb mkpart primary ext3 0% 100% creates=/dev/vdb1
    #   tags: storage

    # - name: storage | format disks
    #   filesystem:
    #     dev: /dev/vdb1
    #     fstype: ext3
    #   tags: storage

    # - name: storage | mount
    #   mount:
    #     name: /hdfs
    #     src: /dev/vdb1
    #     fstype: ext3
    #     state: "{{ item }}"
    #   with_items: [present, mounted]
    #   tags: storage

    - name: storage | fix permissions
      file:
        path: /hdfs
        owner: hadoop
        group: hadoopadmin
        recurse: yes
      tags: storage


- name: hadoop common
  hosts: hadoopnodes
  become: yes
  post_tasks:

    # somehow the permissions get messed up (a subset of the
    # directories become only executable by owner, preventing any
    # other users from calling the hadoop binaries

    - name: fix permissions
      shell: find /opt/hadoop-* -type d -exec chmod a+r,a+x {} +
      tags:
        - hadoop_common
        - hadoop
        - fix_permissions
        - fix

  roles:
    - role: java
    - role: supervisord
    - role: hadoop_install
      hadoop_version: 2.7.1
    - role: hadoop_configure

      hadoop_cfg_path: /opt/hadoop/etc/hadoop

      hadoop_iface: ansible_eth0

      clear_configs:
        - core-site.xml
        - hdfs-site.xml
        - yarn-site.xml
        - mapred-site.xml

      core_site:
        fs.defaultFS: "hdfs://{{ hostvars[groups['namenodes'][0]].ansible_hostname }}"

      mapred_site:
        mapreduce.framework.name: yarn
        mapreduce.jobhistory.address:
          "{{ groups['historyservernodes'][0] }}:10020"

      yarn_site:
        yarn.acl.enable: "true"
        yarn.log-aggregation-enable: "true"

        yarn.resourcemanager.cluster-id: "cluster1"
        yarn.resourcemanager.hostname:
          "{{ hostvars[groups['resourcemanagernodes'][0]].ansible_hostname }}"

        yarn.nodemanager.aux-services: mapreduce_shuffle
        yarn.nodemanager.aux-services.mapreduce_shuffle.class:
          org.apache.hadoop.mapred.ShuffleHandler

      hdfs_site:
        dfs.namenode.name.dir: file:///hdfs/namenode
        dfs.replication: 1
        dfs.permissions.enabled: "true"
        dfs.permissions.superusergroup: hadoop,hadoopadmin
        dfs.namenode.datanode.registration.ip-hostname-check: "false"

        dfs.namenode.rpc-address:
          "{{ hostvars[groups['namenodes'][0]][hadoop_iface].ipv4.address }}:8020"
        dfs.namenode.http-address:
          "{{ hostvars[groups['namenodes'][0]][hadoop_iface].ipv4.address }}:50070"

        dfs.datanode.data.dir: file:///hdfs/datanode


- name: stop everything
  hosts: hadoopnodes
  become: yes
  tasks:

    - name: stop supervisord
      shell: supervisorctl stop all || echo -n
      tags: stop

    - name: stop everything
      shell: killall -q -u hadoop java || echo -n
      tags: stop

## journal nodes need to be up for first_run to execute

- name: journal nodes
  hosts: journalnodes
  become: yes
  roles:

    - role: supervisord
      supervisord_programs:
        journalnode:
          command: sh -lc 'hdfs journalnode'
          user: hadoop
          stdout_logfile: /hdfs/journalnode-stdout.log
          stderr_logfile: /hdfs/journalnode-stderr.log

- name: initialize namenode
  hosts: namenodes[0]
  user: hadoop
  tasks:

    - name: namenode format
      shell: sh -lc 'hdfs namenode -format -force'
      tags: first_run


- name: cleanup
  hosts: namenodes
  user: hadoop
  tasks:
    - name: stop NameNode tasks
      shell: for pid in $(jps | grep NameNode | cut -d' ' -f1); do kill $pid; done
      tags: first_run


- name: namenodes
  hosts: namenodes
  become: yes
  roles:
    - role: supervisord
      supervisord_programs:

        namenode:
          command: sh -lc 'hdfs namenode'
          user: hadoop
          stdout_logfile: /hdfs/namenode-stdout.log
          stderr_logfile: /hdfs/namenode-stderr.log

- name: resourcemanagers
  hosts: resourcemanagernodes
  become: yes
  tags: start_resourcemanager
  roles:
    - role: supervisord
      supervisord_programs:
        resourcemanager:
          command: sh -lc 'yarn resourcemanager'
          user: hadoop
          stdout_logfile: /hdfs/resourcemanager-stdout.log
          stderr_logfile: /hdfs/resourcemanager-stderr.log

- name: data nodes
  hosts: datanodes
  become: yes
  roles:
    - role: supervisord
      supervisord_programs:
        datanode:
          command: sh -lc 'hdfs datanode'
          user: hadoop
          stdout_logfile: /hdfs/datanode-stdout.log
          stderr_logfile: /hdfs/datanode-stderr.log
        nodemanager:
          command: sh -lc 'yarn nodemanager'
          user: hadoop
          stdout_logfile: /hdfs/nodemanager-stdout.log
          stderr_logfile: /hdfs/nodemanager-stderr.log


- name: history server
  hosts: historyservernodes
  become: yes
  roles:
    - role: supervisord
      supervisord_programs:
        historyserver:
          command: sh -lc 'mapred historyserver'
          user: hadoop
          stdout_logfile: /hdfs/historyserver-stdout.log
          stderr_logfile: /hdfs/historyserver-stderr.log
